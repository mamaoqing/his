<?xml version="1.0" ?>
<configuration debug="false">
    <!-- 业务日志：写入kafka -->

<!--    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">-->
<!--        <encoder>-->
<!--            <pattern>[%d] [${appid}][${container_id}][${host_ip}][${PORT}][${AZ}][%t] %F:%L %-5p %c - %m%n</pattern>-->
<!--        </encoder>-->
<!--        <topic>zy</topic>-->
<!--        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" />-->
<!--        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />-->
<!--        <producerConfig>bootstrap.servers=192.168.0.116:9092</producerConfig>-->
<!--        <producerConfig>acks=0</producerConfig>-->
<!--        &lt;!&ndash; wait up to 1000ms and collect log messages before sending them as a batch &ndash;&gt;-->
<!--        <producerConfig>linger.ms=1000</producerConfig>-->
<!--        &lt;!&ndash; even if the producer buffer runs full, do not block the application but start to drop messages &ndash;&gt;-->
<!--        <producerConfig>max.block.ms=0</producerConfig>-->
<!--        &lt;!&ndash; this is the fallback appender if kafka is not available. &ndash;&gt;-->
<!--&lt;!&ndash;        <appender-ref ref="STDOUT" />&ndash;&gt;-->
<!--    </appender>-->

    <!-- 业务日志：写入kafka -->


    <!--  控制台日志  -->
    <appender name="consoleLog" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} ---> %p - %c[%L] %msg%n</pattern>
        </encoder>
    </appender>

    <!--  配置info日志文件  -->
    <appender name="fileInfoLog" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- info 日志，只记录INFO WARN 信息，不记录error信息 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>DENY</onMatch>
            <onMismatch>ACCEPT</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>logs/info-system.%d.log</FileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%d -->%p %c[%L] --> %msg%n</pattern>
            <!--            <pattern>%d{yyyy-MM-dd HH:mm:ss}&ndash;&gt; [%thread] %-5level %logger{50} &ndash;&gt; %msg%n</pattern>-->
        </encoder>
    </appender>

    <!--  配置error日志文件  -->
    <appender name="fileErrorLog" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤只记录error日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <encoder>
            <!-- d:时间 c:类名 L:行号 msg:日志信息 n:换行 p:日志级别 -->
            <pattern>%d -->%p %c[%L] --> %msg%n</pattern>
        </encoder>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>logs/error-system.%d.log</FileNamePattern>
        </rollingPolicy>
    </appender>
    <!-- 全局日志级别 -->
    <root level="info" >
        <appender-ref ref="consoleLog"/>
        <appender-ref ref="fileInfoLog"/>
        <appender-ref ref="fileErrorLog"/>
<!--        <appender-ref ref="KAFKA-EVENTS"/>-->
<!--        <appender-ref ref="kafkaAppender"/>-->
    </root>
</configuration>